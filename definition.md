### 攻击概念

#### 范式

用来度量攻击样本和原样本的差距，一般公式是

$$
||x-x'||_p=(\sum |x-x'|^p)^{\frac{1}{p}}
$$

$L_0$**范式**

即改变了多少个像素

$L_k$**范式**

一般情况下的Minkowski距离

$L_\infty$**范式**

改变最大的那个像素的差

#### Targeted&Nontargeted

Targeted表示欺骗了模型，使其错误地预测对抗性图像为特定标签，Nontargeted欺骗模型至其他非特定标签



这里介绍几个经典的并且较为有效的攻击

### 白箱攻击

#### Box-Constrained L-BFGS

转化为L2无约束凸优化问题

$$
minimize\space\space c||x-x'||^2+J_l(x',y)
$$

$$
c>0,x'\in [0,1]^n
$$

也可用不同的度量方式，比如$L_\infty$。l这里指是不同的类，即同时最小化改变和对抗样本到其他类的Loss，因此也须知Target。另外这个因为可能是二阶问题的缘故采用拟牛顿法，按理GD也是可以解决的。



#### FGSM

FGSM即梯度攻击，梯度攻击目的在于让loss值增大而W，b系数不变，转化为对Loss的无约束凸优化问题。攻击表达式即

$$
x'=x+\epsilon sign(\nabla_xJ(x,y))
$$

让loss值增大而W，b系数不变，因此只能改变输入，我们要让输入朝Loss值变大的方向因此利用Loss对x求导，因此也需要知道Target，但这个是Nontargeted方法（[有篇论文]()上写这是targeted方法，笔者觉得从作者想表述的原理上是不对的，另外这个算法实际targeted和nontargeted都可以做）。这里取sign可能是要自己去调整系数因为毕竟扰动方向是已知的，扰动多大自己调节效果可能会好点，一般还会clip防止扰动超界。我们把FGSM的攻击式看成GD，可以看成FGSM多次更新后的结果，虽然会减慢攻击速度

$$
x_{t+1}=x_t-clip(\epsilon sign(\nabla_xJ(x_t,y)))
$$

另外还有PGD算法也是多次梯度攻击

$$
x_{t+1}=\Pi_{x+S}(x_t+\epsilon sign(\nabla_x J(x_t,y)))
$$

注意FGSM和其变种扰动会随层数越大、线性激活函数越多而增大（毕竟BP滚雪球，另外线性函数不会让改变后的x梯度发生变化，需要保持朝向损失增大的方向）。



#### Deep Fool

先看二分类感知机，我们有扰动向量

$$
r=-\frac{f(x_0)}{||w||_2^2}w
$$

其实这个公式可以理解为样本到分类边界的最短距离

$$
\frac{f(x_0)}{||w||_2}
$$

乘上法线方向的单位向量

$$\frac{w}{||w||_2}$$

，而对于多类感知机，分类正确的凸区域超平面即

$$
P=\cap_c\{x|f_{\hat{k(x_0)}}(x)\ge f_k(x)\}
$$

到某分类决策边界的最小距离

$$
l(x_0)=argmin_{k\neq \hat{k}}\frac{|f_k(x_0)-f_{\hat{k}}(x_0)|}{||w_k-w_{\hat{k}}||_2}
$$

也即最小扰动



#### C&W

我们同样需要让图片和对抗样本差距最小，而分类效果尽可能差，转为凸优化问题，即

$$
min\space ||\delta||_p+c f(x+\delta)
$$

$$
x+\delta \in [0,1]^n
$$

其中f规定小于等于0分类为其他，因此作为优化的一部分，转化扰动为

$$
\delta_i=\frac{1}{2}(tanh(w_i)+1)-x_n
$$

从而定义域可以是实数域，而f函数为

$$
f(x')=max(max(\{Z(x')_i|i\ne t\}-Z(x')_t),-k)
$$

即targeted攻击，使得攻击成t的概率增大，k变大也让分类错的概率增大



